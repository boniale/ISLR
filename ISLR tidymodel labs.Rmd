---
title: "ISLR Tidymodels labs"
author: "Boni M. Ale, MD, MSc, MPH"
date: "2022-08-22"
output: html_document
---
```{r loading packages}
pacman::p_load(
  tidymodels,
  ISLR,
  rpart.plot,
  vip
)
```


```{r loading datasets}
# Boston data set 

data("Boston", package = "MASS")

Boston <- as_tibble(Boston)
```


# 8. Tree- Based Methods
## 8.1. Fitting Classification Trees
We will also use the Carseats data set from the ISLR package to demonstrate a classification model. We create a new variable High to denote if Sales <= 8, then the Sales predictor is removed as it is a perfect predictor of High.
```{r Carseats data set }
Carseats <- as_tibble(
  Carseats
) %>% 
  mutate(
    High = factor(if_else(Sales <= 8, "No", "Yes"))
  ) %>%
  select(-Sales)
```



```{r loading class and reg tree}

## creating a general decision tree specification using rpart as the engine
tree_spec <- decision_tree() %>% 
  set_engine("rpart")

## decision tree specification can be used to create a classification
class_tree_spec <- tree_spec %>% 
  set_mode("classification")
```

```{r model fitting}
class_tree_fit <- class_tree_spec %>% 
  fit(
    High ~ ., data = Carseats
  )

class_tree_fit

# print diagram

class_tree_fit %>% 
  extract_fit_engine() %>% 
  rpart.plot()

#  training accuracy 

augment(class_tree_fit, 
        new_data = Carseats) %>% 
  accuracy(
    truth = High, 
    estimate = .pred_class
  )
```



We can see that the most important variable to predict high sales appears to be shelving location as it forms the first node.

The training accuracy of this model is 85%


Let us take a look at the confusion matrix to see if the balance is there



```{r confusion matrix}
augment(
  class_tree_fit, 
  new_data = Carseats
) %>% 
  conf_mat(
    truth = High,
    estimate = .pred_class
  )
```



And the model appears to work well overall. But this model was fit on the whole data set so we only get the training accuracy which could be misleading if the model is overfitting. Let us redo the fitting by creating a validation split and fit the model on the training data set.
```{r spliting data set}
set.seed(1234)

Carseats_split <- initial_split(Carseats)


Carseats_train <- training(Carseats_split)
Carseats_test <- testing(Carseats_split)
```





Now we can fit the model on the training data set.

```{r fitting model on training carseat data}
class_tree_fit <- class_tree_spec %>% 
  fit(
    High ~ ., data = Carseats_train
  )
```



Let us take a look at the confusion matrix for the training data set and testing data set.
```{r confusion matrix for training and testing data set}
augment(
  class_tree_fit,
  new_data = Carseats_train
) %>% 
  conf_mat(
    truth = High,
    estimate = .pred_class
  )


augment(
  class_tree_fit,
  new_data = Carseats_test
) %>% 
  conf_mat(
    truth = High,
    estimate = .pred_class
  )

```






And what is the accuracy of the model

```{r accuracy}
augment(
  class_tree_fit,
  new_data = Carseats_test
) %>% 
  accuracy(
    truth = High,
    estimate = .pred_class
  )
```



The accuracy get smaller. 

Let us try to tune the cost_complexity of the decision tree to find a more optimal complexity. We use the class_tree_spec object and use the set_args() function to specify that we want to tune cost_complexity. This is then passed directly into the workflow object to avoid creating an intermediate object.

```{r tuning}
class_tree_wf <- workflow() %>% 
  add_model(
    class_tree_spec %>% set_args(
      cost_complexity = tune()
    )
  ) %>% 
  add_formula(
    High ~ .
  )
```
```{r tuning continue}
set.seed(1234)
Carseats_fold <- vfold_cv(Carseats_train)

param_grid <- grid_regular(cost_complexity(range = c(-3, -1)), levels = 10)

tune_res <- tune_grid(
  class_tree_wf, 
  resamples = Carseats_fold, 
  grid = param_grid, 
  metrics = metric_set(accuracy)
)
```



Which values of cost_complexity appear to produce the highest accuracy?

```{r autoplot}
autoplot(tune_res)
```



We can now select the best performing value with select_best(), finalize the workflow by updating the value of cost_complexity and fit the model on the full training data set.

```{r refitting model on training data}
best_complexity <- select_best(tune_res)

class_tree_final <- finalize_workflow(class_tree_wf, best_complexity)

class_tree_final_fit <- fit(class_tree_final, data = Carseats_train)
class_tree_final_fit
```


Model visualisation 
```{r}
class_tree_final_fit %>% 
  extract_fit_engine() %>% 
  rpart.plot()
```



## 8.2 Fitting Regression Trees

Similar to the classification but the main difference here is that the response we are looking at will be continuous instead of categorical.
We can reuse tree_spec as a base for the regression decision tree specification.
We will use Boston dataset here. 
```{r reg}
reg_tree_spec <- tree_spec %>%
  set_mode("regression")
```



Validation split of boston dataset
```{r spliting boston }
set.seed(1234)
Boston_split <- initial_split(Boston)

Boston_train <- training(Boston_split)
Boston_test <- testing(Boston_split)
```



Fitting the model to the training data set

```{r fiting model boston data}
reg_tree_fit <- fit(reg_tree_spec, medv ~ ., Boston_train)
reg_tree_fit

## Accuracy of the model
augment(reg_tree_fit, new_data = Boston_test) %>%
  rmse(truth = medv, estimate = .pred)
```


Printing diagram
```{r print diagram boston }
reg_tree_fit %>%
  extract_fit_engine() %>%
  rpart.plot()
```



Now let us again try to tune the cost_complexity to find the best performing model.


```{r tuning model}
reg_tree_wf <- workflow() %>%
  add_model(reg_tree_spec %>% set_args(cost_complexity = tune())) %>%
  add_formula(medv ~ .)

set.seed(1234)
Boston_fold <- vfold_cv(Boston_train)

param_grid <- grid_regular(cost_complexity(range = c(-4, -1)), levels = 10)

tune_res <- tune_grid(
  reg_tree_wf, 
  resamples = Boston_fold, 
  grid = param_grid
)

autoplot(tune_res)
```



We select the best-performing model according to "rmse" and fit the final model on the whole training data set.
```{r refitting model}
best_complexity <- select_best(tune_res, metric = "rmse")

reg_tree_final <- finalize_workflow(reg_tree_wf, best_complexity)

reg_tree_final_fit <- fit(reg_tree_final, data = Boston_train)
reg_tree_final_fit
```



Model Visualisation
```{r reg_tree model visualtisation}
reg_tree_final_fit %>%
  extract_fit_engine() %>%
  rpart.plot()
```



Visualizing the model reveals a much more complex tree than what we saw in the last section.

# 8.3 Bagging and Random Forests
We  will apply bagging and random forests to the Boston data set. We will be using the randomForest package as the engine.

A bagging model is the same as a random forest where mtry is equal to the number of predictors. We can specify the mtry to be .cols() which means that the number of columns in the predictor matrix is used. This is useful if you want to make the specification more general and useable to many different data sets. .cols() is one of many descriptors in the parsnip package. We also set importance = TRUE in set_engine() to tell the engine to save the information regarding variable importance. This is needed for this engine if we want to use the vip package later.
```{r bagging model}
bagging_spec <- rand_forest(mtry = .cols()) %>%
  set_engine("randomForest", importance = TRUE) %>%
  set_mode("regression")
```

